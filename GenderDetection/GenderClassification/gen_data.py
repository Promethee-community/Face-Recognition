from __future__ import print_function
import argparse
import os

import numpy as np
import sklearn
import pickle
from face_recognition import face_locations
from PIL import Image, ImageDraw, ImageFont
from tqdm import tqdm
import cv2
import pandas as pd
# we are only going to use 4 attributes
#sklearn.neural_network.multilayer_perceptron()
COLS = ['Male', 'Asian', 'White', 'Black']
N_UPSCLAE = 1

import face_recognition
import scipy.misc
from scipy.special import comb

def extract_features(img_path):
    """Exctract 128 dimensional features
    """
    X_img = face_recognition.load_image_file(img_path)
    #print(X_img)
    #print(X_img.shape)
    #print(img_path)
    #print(X_img.shape[0])
    #print(X_img.shape[1])
    locs = face_locations(X_img, number_of_times_to_upsample = N_UPSCLAE)
    #print(locs)
    if len(locs) == 0:
        return None, None
    face_encodings = face_recognition.face_encodings(X_img, known_face_locations=locs)
    return face_encodings, locs

def predict_one_image(img_path, clf, labels):
    """Predict face attributes for all detected faces in one image
    """
    face_encodings, locs = extract_features(img_path)
    if not face_encodings:
        return None, None
    pred = pd.DataFrame(clf.predict_proba(face_encodings),
                        columns = labels)
    pred = pred.loc[:, COLS]
    return pred, locs
def draw_attributes(img_path, df):
    """Write bounding boxes and predicted face attributes on the image
    """
    img = cv2.imread(img_path)
    # img  = cv2.cvtColor(color, cv2.COLOR_BGR2RGB)
    for row in df.iterrows():
        top, right, bottom, left = row[1][4:].astype(int)
        if row[1]['Male'] >= 0.5:
            gender = 'Male'
        else:
            gender = 'Female'

        race = np.argmax(row[1][1:4])
        text_showed = "{} {}".format(race, gender)

        cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)
        font = cv2.FONT_HERSHEY_DUPLEX
        img_width = img.shape[1]
        cv2.putText(img, text_showed, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)
    return img

def get_labels(csv_name,colname):
  labels_dic={}
  with open(csv_name) as f_csv: #compares test results with wondonghyeon's dataset
    content_csv = f_csv.readlines()
    label_csv=content_csv[0].strip().split(",")
    for i in range(0,len(label_csv)):
      if label_csv[i]==colname:
          col_pose_csv=i #index to recover "gender" data only, on results generated by pred.py
    for ln in range(1,len(content_csv)): 
        data_csv=content_csv[ln].strip().split(",")
        labels_dic[data_csv[0]]=data_csv[col_pose_csv]
  return labels_dic

      
def gen_data(data_dir,X_name,Y_name):

    labels=[]
    file_lst=[]
    X=[]
    Y=[]
    for (td,ln) in data_dir:
        gender_dic=get_labels(ln,"Male")
        for file_name in os.listdir(td):
            if file_name.endswith(".jpg"):
                file_lst.append((os.path.join(td,file_name),gender_dic[file_name]))

    for fname in tqdm(file_lst):
        data,loc=extract_features(fname[0])

        if  data is not None and (len(loc)==1):
          X.append(data[0])
          if int(fname[1])>0:
            Y.append(1.0)
          else:
            Y.append(0.0)

    np.save(X_name, X)
    np.save(Y_name, Y)